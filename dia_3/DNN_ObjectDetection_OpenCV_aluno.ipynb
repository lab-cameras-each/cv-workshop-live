{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "colab": {
      "name": "DNN_ObjectDetection_OpenCV_aluno.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed62bc19"
      },
      "source": [
        "<center><h1> Usando Modelos de Deep Learning com apoio do OpenCV </h1></center>\n",
        "<center><h1> Detecção de Objetos </h1></center>"
      ],
      "id": "ed62bc19"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PZsPV78a20N"
      },
      "source": [
        "<h2>1 - As bibliotecas/pacotes pré-requisitos para este workshop são:</h2>\n",
        "\n",
        "<ul>\n",
        "    <li>OpenCV</li>\n",
        "    <li>Matplotlib</li>\n",
        "    <li>Numpy</li>\n",
        "</ul>\n",
        "<h3>Inicialmente, será necessário atualizar a versão do OpenCV para 4.5 ou maior.\n",
        "<ul>\n",
        "    <li>Executar a célula abaixo (que contém o comando 'pip install opencv-python --upgrade')</li>\n",
        " \n",
        "</ul>\n",
        "\n"
      ],
      "id": "1PZsPV78a20N"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uA8az18zKlqE"
      },
      "source": [
        "!pip install opencv-python --upgrade"
      ],
      "id": "uA8az18zKlqE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCco8XaPK69J"
      },
      "source": [
        "<ul>\n",
        "    <li>Importar a biblioteca do OpenCV e confirmar a versão que está sendo usada é 4.5 ou maior.</li>\n",
        " \n",
        "</ul>\n"
      ],
      "id": "kCco8XaPK69J"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWokrvu-K3fR"
      },
      "source": [
        "# Importar o OpenCV\n",
        "import cv2\n",
        "print(\"OpenCV version:\", cv2.__version__)"
      ],
      "id": "aWokrvu-K3fR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7PMTZe4bhPg"
      },
      "source": [
        "# Importar as demais bibliotecas/pacotes necessários ao projeto\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import requests\n",
        "from google.colab.patches import cv2_imshow \n",
        "\n",
        "## Acesso ao google drive a partir do colab\n",
        "from google.colab import drive\n",
        "#drive.mount('/content/gdrive') \n",
        "drive.mount(\"/content/gdrive\", force_remount=True)\n",
        "\n",
        "## IMPORTANTE:\n",
        "## Pré-requisitos para a configuração de acesso aos recursos do workshop:\n",
        "## 1. A pasta 'recursos_workshop' deve ter sido compartilhada com o seu usuário do Google Drive\n",
        "## 2. O aluno deverá fazer acesso à pasta compartilhada em sua conta de Google Drive\n",
        "## 3. O aluno deverá criar um atalho (shortcut) para a pasta compartilhada. \n",
        "##    Este atalho ficará localizado no próprio drive do aluno ('Meu Drive' ou 'MyDrive') e\n",
        "##    terá o nome 'recursos_workshop'\n",
        "\n",
        "\n",
        "## Caminho para a pasta de recursos do workshop\n",
        "resources_path = \"gdrive/MyDrive/recursos_workshop/\"\n",
        "\n",
        "## Caminho para a pasta de modelos de Deep Learning\n",
        "models_path = resources_path + \"modelos_DL/\"\n",
        "\n",
        "## Caminho para a pasta de imagens \n",
        "image_dir = resources_path + \"imagens\"\n",
        "\n"
      ],
      "id": "S7PMTZe4bhPg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jm3dX84buzY"
      },
      "source": [
        "## 2 - Configurações: diretórios de modelos, imagens, notebooks \n",
        "#### definições/configurações de modelos de Deep Learning e pesos pré-treinados: \n",
        "<ul>\n",
        "<li>Estão localizados na pasta 'recursos_workshop' do Google drive</li>\n",
        "</ul>\n",
        "\n",
        "#### imagens:\n",
        "<ul>  \n",
        "    <li>Também estão no google drive ou, dependendo do caso, serão baixadas da internet</li>\n",
        "</ul>\n",
        "    \n"
      ],
      "id": "-jm3dX84buzY"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b6ef0a4"
      },
      "source": [
        "## 3 - Leitura de imagens e pré-processamento \n",
        "### 3.1 - A imagem pode ser lida do disco usando o método \"imread\" do OpenCV\n",
        "<ul>\n",
        "    <li>cv2.imread</li>    \n",
        "</ul>\n",
        "\n",
        "### 3.2 - O método \"blobFromImage\" permite fazer resize, crop, scaling, normalizing, mudar de RGB para BGR.\n",
        "### 3.3 - Este método produz um BLOB de 4 dimensões \n",
        "<ul>\n",
        "    <li>cv2.dnn.blobFromImage</li>\n",
        "    <li>cv2.dnn.blobFromImages</li>    \n",
        "</ul>\n"
      ],
      "id": "4b6ef0a4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a20324c"
      },
      "source": [
        "## 4 - Frameworks e modelos\n",
        "### 4.1 - Os seguintes frameworks são suportados pelo módulo <a href=\"https://github.com/opencv/opencv/tree/master/modules/dnn\" target=\"_blank\" rel=\"noopener noreferrer\">DNN</a> do OpenCV\n",
        "\n",
        "<ul>\n",
        "<li><a href=\"http://caffe.berkeleyvision.org/\" target=\"_blank\" rel=\"noopener noreferrer\">Caffe</a></li>\n",
        "<li><a href=\"https://www.tensorflow.org/\" target=\"_blank\" rel=\"noopener noreferrer\">Tensorflow</a></li>\n",
        "<li><a href=\"http://torch.ch/\" target=\"_blank\" rel=\"noopener noreferrer\">Torch</a></li>\n",
        "<li><a href=\"https://pjreddie.com/darknet/\" target=\"_blank\" rel=\"noopener noreferrer\">Darknet</a></li>\n",
        "<li><a href=\"https://onnx.ai/\" target=\"_blank\" rel=\"noopener noreferrer\">ONNX</a></li>\n",
        "</ul>    \n"
      ],
      "id": "2a20324c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "990c081c"
      },
      "source": [
        "## 5 - Carga em memória de modelos de DNN (a partir de modelo serializado em disco)\n",
        "### OpenCV usa modelos pré-treinados em datasets com acesso público (por exemplo, ImageNet). Esses modelos são desenvolvidos com o uso de diversos frameworks (Caffe, Tensorflow, Pytorch, etc.)\n",
        "<ul>\n",
        "    <li>cv2.dnn.readNetFromCaffe</li>\n",
        "    <li>cv2.dnn.readNetFromDarknet</li>\n",
        "    <li>cv2.dnn.readNetFromTensorFlow</li>\n",
        "    <li>cv2.dnn.readNetFromTorch</li>\n",
        "    <li>cv2.dnn.readNetFromONNX</li>\n",
        "</ul>"
      ],
      "id": "990c081c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89d002b6"
      },
      "source": [
        "## 6 - Inferência usando o modelo\n",
        "### 6.1 - Definindo o input para o modelo e iniciando a inferência\n",
        "\n",
        "<ul>\n",
        "<li>setInput(blob)</li>\n",
        "<li>forward</li>\n",
        "</ul>"
      ],
      "id": "89d002b6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a1a2bc6"
      },
      "source": [
        "## 7 - Detecção de objetos\n",
        "### A detecção de objetos envolve a classificação e localização dos diferentes objetos contidos em uma imagem, com base em um conjunto pré-definido de classes.\n",
        "### Logo abaixo, iremos definir algumas funções de apoio para as nossas tarefas.\n",
        "### Utilizando o modelo Faster-RCNN (desenvolvido com uso do framework Tensorflow) efetuar a tarefa de detecção de objetos nas imagens definidas abaixo e apresentar os resultados\n"
      ],
      "id": "7a1a2bc6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3g2Lor58eLDa"
      },
      "source": [
        "### 7.1 - Carregar em memória o modelo, a partir dos arquivos 'model' e 'config'"
      ],
      "id": "3g2Lor58eLDa"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhbxRqoFeLDm"
      },
      "source": [
        "def getNet(faster_rcnn_dir, fmodel, fconfig):\n",
        "    model = os.path.join(faster_rcnn_dir, fmodel) # binary protobuf description of the network architecture\n",
        "    config = os.path.join(faster_rcnn_dir, fconfig) # .pbtxt file that contains text graph definition in protobuf format.\n",
        "    net = cv2.dnn.readNetFromTensorflow(model, config)\n",
        "    return net"
      ],
      "id": "lhbxRqoFeLDm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bef3331"
      },
      "source": [
        "### 7.2 - Setar na variável CLASSES os nomes das classes que foram utilizadas no treinamento deste modelo"
      ],
      "id": "4bef3331"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5231bcef"
      },
      "source": [
        "# Classes: MS-COCO dataset com 80 classes\n",
        "CLASSES = [\"person\",\"bicycle\",\"car\",\"motorbike\",\"aeroplane\",\"bus\",\"train\",\"truck\",\"boat\",\n",
        "           \"traffic light\",\"fire hydrant\",\"stop sign\",\"parking meter\",\"bench\",\"bird\",\"cat\",\n",
        "           \"dog\",\"horse\",\"sheep\",\"cow\",\"elephant\",\"bear\",\"zebra\",\"giraffe\",\"backpack\",\n",
        "           \"umbrella\",\"handbag\",\"tie\",\"suitcase\",\"frisbee\",\"skis\",\"snowboard\",\n",
        "           \"sports ball\",\"kite\",\"baseball bat\",\"baseball glove\",\"skateboard\",\n",
        "           \"surfboard\",\"tennis racket\",\"bottle\",\"wine glass\",\"cup\",\"fork\",\"knife\",\n",
        "           \"spoon\",\"bowl\",\"banana\",\"apple\",\"sandwich\",\"orange\",\"broccoli\",\"carrot\",\n",
        "           \"hot dog\",\"pizza\",\"donut\",\"cake\",\"chair\",\"sofa\",\"pottedplant\",\"bed\",\n",
        "           \"diningtable\",\"toilet\",\"tvmonitor\",\"laptop\",\"mouse\",\"remote\",\"keyboard\",\n",
        "           \"cell phone\",\"microwave\",\"oven\",\"toaster\",\"sink\",\"refrigerator\",\"book\",\"clock\",\n",
        "           \"vase\",\"scissors\",\"teddy bear\",\"hair drier\",\"toothbrush\"]\n",
        "\n",
        "# Definimos aqui um array de colorações para ser usado no desenho das bounding boxes\n",
        "COLORS = np.random.uniform(245, 255, size=(len(CLASSES), 3)) #np.random.uniform(0, 255, size=(len(CLASSES), 3))\n"
      ],
      "id": "5231bcef",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "650f3aa9"
      },
      "source": [
        "### 7.3 - Usando OpenCV, efetuar a leitura da imagem contida no path/arquivo 'filePath'"
      ],
      "id": "650f3aa9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fc64f623"
      },
      "source": [
        "def imageResize(frame, maxH):\n",
        "    h, w = frame.shape[:2] # obter a altura e largura do frame\n",
        "    if maxH < h:\n",
        "        aspect_ratio = w/h\n",
        "        blob_height = maxH \n",
        "        blob_width = int(blob_height * aspect_ratio)\n",
        "        dsize = (blob_width, blob_height) # keep the frame's original aspect ratio\n",
        "        return cv2.resize(frame, dsize)\n",
        "    return frame\n",
        "\n",
        "def readImage(filePath):\n",
        "    frame = cv2.imread(filePath)\n",
        "    heightMax = 1600  # max height used here to do a imshow inside Colab\n",
        "    out = imageResize(frame, heightMax)\n",
        "    return out"
      ],
      "id": "fc64f623",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e80ee661"
      },
      "source": [
        "### 7.4 - Obter o blob a partir da imagem lida pelo OpenCV, aplicando pré-processamento. O tipo de pré-processamento a ser utilizado irá depender de cada modelo de rede utilizado."
      ],
      "id": "e80ee661"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "289b992c"
      },
      "source": [
        "def getBlobFromFrame(frame, H, W, swapRB, crop):\n",
        "    \n",
        "    blob = cv2.dnn.blobFromImage(frame, size=(H,W), swapRB=swapRB, crop=crop)\n",
        "\n",
        "    return blob"
      ],
      "id": "289b992c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc5edb69"
      },
      "source": [
        "### 7.5 - Definir a imagem lida como entrada para o processamento do modelo"
      ],
      "id": "bc5edb69"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7de05efb"
      },
      "source": [
        "def setNetInput(net, blob):\n",
        "    #set input \n",
        "    net.setInput(blob)\n",
        "    return net   "
      ],
      "id": "7de05efb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff099531"
      },
      "source": [
        "### 7.6 - Processar a inferência neste modelo, usando esta imagem "
      ],
      "id": "ff099531"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06ca8a53"
      },
      "source": [
        "# O blob da imagem a ser usado como input para o modelo realizar a inferência\n",
        "# precisa já ter sido definido antes de acionar esta função\n",
        "def processInference(net):\n",
        "    #forward\n",
        "    predictions = net.forward()\n",
        "    return predictions"
      ],
      "id": "06ca8a53",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d10bfc9"
      },
      "source": [
        "### 7.7 - Efetuar a detecção de objetos (inferência) usando um modelo já carregado em memória"
      ],
      "id": "9d10bfc9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5349af93"
      },
      "source": [
        "def getBlobAndDetectObjects(net, frame, H, W, swapRB, crop, confidence, threshold):               \n",
        "               \n",
        "    boxes = []\n",
        "    confidences = []\n",
        "    \n",
        "    height = frame.shape[0]\n",
        "    width = frame.shape[1]\n",
        "    \n",
        "    blob = getBlobFromFrame(frame, H, W, swapRB, crop)\n",
        "\n",
        "    net = setNetInput(net, blob)\n",
        "    \n",
        "    detections = processInference(net)\n",
        "\n",
        "    classes = []\n",
        "    \n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    for detection in detections[0, 0, :, :]:\n",
        "        class_id = int(detection[1])\n",
        "        \n",
        "        score = float(detection[2])\n",
        "\n",
        "        if score > confidence:\n",
        "            left = detection[3] * width\n",
        "            top = detection[4] * height\n",
        "            right = detection[5] * width\n",
        "            bottom = detection[6] * height\n",
        "            box = [int(left), int(top), int(right), int(bottom)]\n",
        "\n",
        "            confidences.append(score)\n",
        "            classes.append(class_id)\n",
        "            boxes.append(box)\n",
        "    \n",
        "    \n",
        "    #using cv2 non maximum suppression\n",
        "    nms_inds = cv2.dnn.NMSBoxes(boxes, confidences, confidence, threshold)\n",
        "    \n",
        "    nms_inds1 = [i[0] for i in nms_inds] # get the indexes\n",
        "    \n",
        "    filtered_boxes =  np.array(boxes)[nms_inds1]\n",
        "    probs =  np.array(confidences)[nms_inds1]\n",
        "   \n",
        " \n",
        "    \n",
        "    final_boxes = []\n",
        "    final_confidences = []\n",
        "    final_classes = []\n",
        "\n",
        "    for index, box in enumerate(filtered_boxes):\n",
        "\n",
        "        startX, startY, endX, endY = box\n",
        "        box = [int(startX), int(startY), int(endX), int(endY)]\n",
        "\n",
        "        final_boxes.append(box)\n",
        "        final_confidences.append(probs[index])\n",
        "        final_classes.append(classes[index])   ###check this lps\n",
        "\n",
        "\n",
        "    return final_boxes, final_confidences, final_classes\n",
        "    \n"
      ],
      "id": "5349af93",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aa236dcf"
      },
      "source": [
        "### 7.8 - Desenhar na imagem os marcadores (bounding boxes) para cada uma das classes detectadas"
      ],
      "id": "aa236dcf"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3e0e8a94"
      },
      "source": [
        "def drawBoxes(frame, boxes, confidences, classes):\n",
        "    for i in range(0, len(boxes)):\n",
        "        ind = classes[i] - 1\n",
        "        (startX, startY, endX, endY) = boxes[i]\n",
        "        y = startY - 15 if startY - 15 > 15 else startY + 15\n",
        "        text = \"{}: {:.2f}%\".format(CLASSES[ind], confidences[i] * 100)    \n",
        "        cv2.rectangle(frame, (startX, startY), (endX, endY), COLORS[i], 2)\n",
        "        cv2.putText(frame, text, (startX, y), cv2.FONT_HERSHEY_SIMPLEX, 0.7, COLORS[ind], 2)\n",
        "    return frame"
      ],
      "id": "3e0e8a94",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd5ca2da"
      },
      "source": [
        "### 7.9 - Exibir a imagem usando OpenCV"
      ],
      "id": "dd5ca2da"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "083ce423"
      },
      "source": [
        "def displayImage(image, imgTitle=\"Image\"): \n",
        "    cv2_imshow(image)\n"
      ],
      "id": "083ce423",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vV0MjRlZkfsr"
      },
      "source": [
        "### 7.10 - Efetuar o processo de detecção de objetos utilizando o modelo, uma imagem lida via OpenCV, parâmetros de pré-processamento da imagem e os limites de confiança e threshold"
      ],
      "id": "vV0MjRlZkfsr"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JXYFgCMkfs1"
      },
      "source": [
        "def procDetect(image_dir, image_file, net, H, W, swapRB, crop, confidence, threshold):\n",
        "  # Read img from disk\n",
        "  frame = readImage(os.path.join(image_dir, image_file))\n",
        "\n",
        "  boxes, confidences, classes = getBlobAndDetectObjects(net, frame, H, W, swapRB, crop, confidence, threshold)\n",
        "\n",
        "  # Desenhar os marcadores (bounding boxes) para cada uma das classes detectadas \n",
        "  frame = drawBoxes(frame, boxes, confidences, classes)\n",
        "\n",
        "  # exibir a imagem\n",
        "  displayImage(frame)\n"
      ],
      "id": "2JXYFgCMkfs1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2035958"
      },
      "source": [
        "### 7.11 - Acionamento do processo de detecção de objetos utilizando as funções de apoio\n",
        "#### 7.11.1 - Definições de configuração: diretórios de modelos, imagens,  arquivos de modelos e parâmetros de pré-processamento de imagens \n"
      ],
      "id": "f2035958"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pF-XvVnVe9gQ"
      },
      "source": [
        "# diretório contendo os arquivos do modelo Faster-RCNN \n",
        "faster_rcnn_dir = models_path + 'Faster_RCNN/faster_rcnn_inception_v2_coco_2018_01_28'\n",
        "\n",
        "# binary protobuf description of the network architecture\n",
        "fmodel = 'frozen_inference_graph.pb' \n",
        "\n",
        "# .pbtxt file that contains text graph definition in protobuf format.\n",
        "fconfig =  'graph.pbtxt'\n",
        "\n",
        "H = 300  # height\n",
        "W = 300  # width\n",
        "swapRB=True\n",
        "crop=False\n",
        "confidence = 0.4 # só serão aceitas detecções maiores que este valor\n",
        "threshold = 0.5  # valor a ser aplicado como threshold no non-max suppression\n",
        "                 # são aceitas bounding boxes com overlapping maior que este valor\n",
        "\n",
        "##"
      ],
      "id": "pF-XvVnVe9gQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8f866f14"
      },
      "source": [
        "### 7.12 - Efetuar a carga do modelo em memória"
      ],
      "id": "8f866f14"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0f7268d6"
      },
      "source": [
        "net = getNet(faster_rcnn_dir, fmodel, fconfig) \n"
      ],
      "id": "0f7268d6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_0Wm1qkl6xP"
      },
      "source": [
        "### 7.13 - Efetuar a detecção processo para uma imagem"
      ],
      "id": "5_0Wm1qkl6xP"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7h4NUyDmHJk"
      },
      "source": [
        "image_file = 'horses.jpg'\n",
        "\n",
        "procDetect(image_dir, image_file, net, H, W, swapRB, crop, confidence, threshold)\n",
        " "
      ],
      "id": "n7h4NUyDmHJk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDLxT462Ymdm"
      },
      "source": [
        "### Atividade para o aluno\n",
        "#### 7.14 - Exercício: utilizando o modelo já carregado, efetuar a detecção de objetos para as seguintes imagens\n",
        "<ul>\n",
        "<li>pexels-engin-akyurt-1769271.jpg</li>\n",
        "<li>giraffe.jpg</li>\n",
        "</ul>"
      ],
      "id": "mDLxT462Ymdm"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f57d9e2c"
      },
      "source": [
        "### 7.15 - Usando outros modelos para a detecção de objetos"
      ],
      "id": "f57d9e2c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f475342c"
      },
      "source": [
        "#### 7.15.1 - Utilizando o modelo MobileNetSSD (desenvolvido com uso do framework Caffe) efetuar a tarefa de detecção de objetos nas imagens definidas abaixo e apresentar os resultados"
      ],
      "id": "f475342c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79a12630"
      },
      "source": [
        "# diretório contendo os arquivos do modelo MobileNetSSD \n",
        "mobile_ssd_dir = models_path + 'Mobile_SSD'  \n",
        "\n",
        "# MobileNet-SSD \n",
        "# MobileNetSSD_deploy.caffemodel: the model\n",
        "# MobileNetSSD_deploy.prototxt: the text file that describes the model's parameters\n",
        "\n",
        "fmodel_file = 'MobileNetSSD_deploy.caffemodel' # the pre-trained model\n",
        "fconfig =  'MobileNetSSD_deploy.prototxt' # the text file that describes the model's parameters\n"
      ],
      "id": "79a12630",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49e34a6e"
      },
      "source": [
        "#### 7.15.2 - Classes - Dataset Pascal VOC - com 20 classes"
      ],
      "id": "49e34a6e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9f564630"
      },
      "source": [
        "class_labels = [\"background\", 'airplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus',\n",
        "    'car', 'cat', 'chair', 'cow', 'dining table', 'dog',\n",
        "    'horse', 'motorbike', 'person', 'potted plant', 'sheep',\n",
        "    'sofa', 'train', 'TV or monitor']\n"
      ],
      "id": "9f564630",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d33a6fc"
      },
      "source": [
        "#### 7.15.3 - Fazer a carga do modelo em memória"
      ],
      "id": "8d33a6fc"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bf69f012"
      },
      "source": [
        "def getCaffeModel(mobile_ssd_dir, fconfig, fmodel_file):\n",
        "    model_file = os.path.join(mobile_ssd_dir, fmodel_file) # the pre-trained model\n",
        "    config = os.path.join(mobile_ssd_dir, fconfig) # the text file that describes the model's parameters\n",
        "    model = cv2.dnn.readNetFromCaffe(config, model_file)\n",
        "    return model"
      ],
      "id": "bf69f012",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c5fd9bc"
      },
      "source": [
        "#### 7.15.4 -  Efetuar o pré-processamento da imagem lida e obter o blob que será processado pelo modelo - método A"
      ],
      "id": "7c5fd9bc"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6f2058ed"
      },
      "source": [
        "def getBlobFromImageMobileSSD_pre_A(frame):\n",
        "    blob = cv2.dnn.blobFromImage(cv2.resize(frame, (300, 300)), 0.007843, (300, 300), 127.5)\n",
        "    return blob"
      ],
      "id": "6f2058ed",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e62e8afe"
      },
      "source": [
        "#### 7.15.5 - Efetuar o pré-processamento da imagem lida e obter o blob que será processado pelo modelo - método B"
      ],
      "id": "e62e8afe"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f157a2e1"
      },
      "source": [
        "def getBlobFromImageMobileSSD_pre_B(frame):\n",
        "    h, w = frame.shape[:2]\n",
        "    aspect_ratio = w/h\n",
        "    blob_height = 300\n",
        "    blob_width = int(blob_height * aspect_ratio)\n",
        "    blob_size = (blob_width, blob_height) # keep the frame's original aspect ratio\n",
        "    color_scale = 1.0/127.5  ## 0.007843\n",
        "    average_color = (127.5, 127.5, 127.5)\n",
        "    blob = cv2.dnn.blobFromImage(frame, scalefactor=color_scale, size=blob_size,  mean=average_color)\n",
        "    return blob"
      ],
      "id": "f157a2e1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7300ea8"
      },
      "source": [
        "#### 7.15.6 - Definir o blob como dado de entrada para o modelo e efetuar a detecção de objetos"
      ],
      "id": "e7300ea8"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1287e2d"
      },
      "source": [
        "def modelSetInputAndDetectObjects(model, blob):\n",
        "    model.setInput(blob)\n",
        "    results = model.forward()\n",
        "    return results"
      ],
      "id": "d1287e2d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50e49f13"
      },
      "source": [
        "#### 7.15.7 - Para todas as detecções com índice de confiança maior que o limite informado, desenhar as correspondentes 'bounding-boxes' com seus índices de confiança"
      ],
      "id": "50e49f13"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dedeab01"
      },
      "source": [
        "def getBoxesAndConfidences(iframe, results, labels, conf_threshold):\n",
        "    h, w = iframe.shape[:2]\n",
        "    frame = iframe.copy()\n",
        "    for object in results[0, 0]:\n",
        "        confidence = object[2]\n",
        "        if confidence > conf_threshold:\n",
        "\n",
        "            # Get the object's coordinates.\n",
        "            x0, y0, x1, y1 = (object[3:7] * [w, h, w, h]).astype(int)\n",
        "\n",
        "            # Get the classification result.\n",
        "            id = int(object[1])\n",
        "            label = labels[id] ## id - 1\n",
        "            # Draw a bounding box\n",
        "            cv2.rectangle(frame, (x0, y0), (x1, y1),\n",
        "                          (255, 255, 0), 2)\n",
        "\n",
        "            # Inform class and confidence \n",
        "            yt = y0 - 15 if y0 - 15 > 15 else y0 + 15\n",
        "            text = '%s (%.1f%%)' % (label, confidence * 100.0)\n",
        "            # cv2.putText(frame, text, (x0, y0 - 20),\n",
        "            #    cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 0), 2) \n",
        "            cv2.putText(frame, text, (x0, yt),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 0), 2) \n",
        "    return frame"
      ],
      "id": "dedeab01",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoL1EmfZBwCM"
      },
      "source": [
        "#### 7.15.8 - Processar a detecção e exibir resultados"
      ],
      "id": "EoL1EmfZBwCM"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXGYKdijA1Ao"
      },
      "source": [
        "def detectAndShowResults(image_dir, image_file, model, class_labels, conf_threshold):\n",
        "\n",
        "    # Read img from disk\n",
        "    frame = readImage(os.path.join(image_dir, image_file))\n",
        "\n",
        "    # get blob method 1 - blob_A\n",
        "    blob_A = getBlobFromImageMobileSSD_pre_A(frame)\n",
        "\n",
        "    # get blob method 2 - blob_B\n",
        "    blob_B = getBlobFromImageMobileSSD_pre_B(frame)\n",
        "\n",
        "    # aqui, vamos ver os resultados para dois tipos diferentes de pré-processamento da imagem\n",
        "    results_A = modelSetInputAndDetectObjects(model, blob_A)\n",
        "    results_B = modelSetInputAndDetectObjects(model, blob_B)\n",
        " \n",
        "    # put results in the frames \n",
        "    frame_A = getBoxesAndConfidences(frame, results_A, class_labels, conf_threshold)    \n",
        "    frame_B = getBoxesAndConfidences(frame, results_B, class_labels, conf_threshold)    \n",
        "    \n",
        "    # show results\n",
        "    print(\"\\nPre-processamento A\")\n",
        "    displayImage(frame_A, \"Pre-proc A\")\n",
        "    print(\"\\nPre-processamento B\")\n",
        "    displayImage(frame_B, \"Pre-Proc B\")\n",
        "\n",
        "    "
      ],
      "id": "FXGYKdijA1Ao",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dda77eeb"
      },
      "source": [
        "#### 7.15.8 - O processo de deteção: carregar o modelo, obter a imagem a ser processada, realizar a inferência (deteção de objetos) e exibir os resultados"
      ],
      "id": "dda77eeb"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iGn2jL7CGNo"
      },
      "source": [
        "model = getCaffeModel(mobile_ssd_dir, fconfig, fmodel_file) \n",
        "conf_threshold = 0.5   \n",
        "\n",
        "image_file = 'dog.jpg'\n",
        "\n",
        "detectAndShowResults(image_dir, image_file, model, class_labels, conf_threshold)\n",
        "    "
      ],
      "id": "-iGn2jL7CGNo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSzqgGWHGH7D"
      },
      "source": [
        "### Atividade para o aluno\n",
        "#### 7.15.9 - Exercício: utilizando o modelo já carregado, efetuar a detecção de objetos para as seguintes imagens\n",
        "<ul>\n",
        "<li>person.jpg</li>\n",
        "<li>giraffe.jpg</li>\n",
        "</ul>"
      ],
      "id": "wSzqgGWHGH7D"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebb3d90e"
      },
      "source": [
        "### Atividade para o aluno\n",
        "#### 7.15.10 - Utilizando o modelo  Faster-RCNN ResNet-50  (desenvolvido com uso do framework Tensorflow) efetuar a tarefa de detecção de objetos nas imagens definidas abaixo e apresentar os resultados\n",
        "<ul>\n",
        "<li>horses.jpg</li>\n",
        "<li>giraffe.jpg</li>\n",
        "<li>dog.jpg</li>\n",
        "</ul>\n"
      ],
      "id": "ebb3d90e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41cf05b4"
      },
      "source": [
        "# diretório contendo os arquivos do modelo Faster-RCNN ResNet-50 \n",
        "faster_rcnn_resnet50_dir = models_path + 'Faster_RCNN/faster_rcnn_resnet50_coco_2018_01_28'\n",
        "\n",
        "\n",
        "fmodel = 'frozen_inference_graph.pb' # the pre-trained model\n",
        "fconfig = 'faster_rcnn_resnet50_coco_2018_01_28.pbtxt' # the text file that describes the model's parameters\n",
        "\n",
        "# lembrando que usaremos as mesmas classes do caso de uso anterior com a rede faster_rcnn (treinada com o dataset coco)\n",
        "# As classes estão definidas na variável CLASSES\n",
        "\n",
        "H = 300  # height\n",
        "W = 300  # width\n",
        "swapRB=True\n",
        "crop=False\n",
        "confidence = 0.4 # só serão aceitas detecções maiores que este valor\n",
        "threshold = 0.5  # valor a ser aplicado como threshold no non-max suppression\n",
        "                 # são aceitas bounding boxes com overlapping maior que este valor\n",
        "\n",
        "##"
      ],
      "id": "41cf05b4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "ef66a0d5"
      },
      "source": [
        "# implementar a chamada à função getNet\n"
      ],
      "id": "ef66a0d5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yl63lKt2K-sy"
      },
      "source": [
        "#### 7.15.11 - Utilizando o modelo YOLOV3 (desenvolvido com uso do framework Darknet) efetuar a tarefa de detecção de objetos nas imagens definidas abaixo e apresentar os resultados"
      ],
      "id": "Yl63lKt2K-sy"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAdq1otWK-sz"
      },
      "source": [
        "# diretório contendo os arquivos do modelo YOLOV3 \n",
        "yolov3_dir = models_path +  'Darknet/darknet_detect/yolov3'\n",
        "\n",
        "yolov3_cfg = 'yolov3.cfg'\n",
        "yolov3_weights = 'yolov3.weights'\n",
        "\n",
        "# classes\n",
        "class_dir  = models_path + 'Darknet/darknet_detect'\n",
        "class_file = \"coco.names\""
      ],
      "id": "uAdq1otWK-sz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pg0igSnRMQ9N"
      },
      "source": [
        "#### 7.15.12 - Carga das classes"
      ],
      "id": "pg0igSnRMQ9N"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24db6c34"
      },
      "source": [
        "# carga das classes\n",
        "classes = open(os.path.join(class_dir, class_file)).read().strip().split('\\n')\n",
        "np.random.seed(42)\n",
        "colors = np.random.randint(0, 255, size=(len(classes), 3), dtype='uint8')"
      ],
      "id": "24db6c34",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YKNnCXZNAUe"
      },
      "source": [
        "#### 7.15.13 - Carga do modelo em memória"
      ],
      "id": "8YKNnCXZNAUe"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f45af2b2"
      },
      "source": [
        "def loadYOLOModel(yolo_dir, yolo_cfg, yolo_weights):\n",
        "    cfg = os.path.join(yolo_dir, yolo_cfg)\n",
        "    weights = os.path.join(yolo_dir, yolo_weights)\n",
        "    dnet = cv2.dnn.readNetFromDarknet(cfg, weights)\n",
        "    dnet.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n",
        "    return dnet\n",
        "\n",
        "def getYOLONetAndLN(yolo_dir, yolo_cfg, yolo_weights):\n",
        "    # load YOLO net\n",
        "    net = loadYOLOModel(yolo_dir, yolo_cfg, yolo_weights)\n",
        "    ln = net.getLayerNames()\n",
        "    ln = [ln[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
        "    return net, ln    "
      ],
      "id": "f45af2b2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1JLygvON0gv"
      },
      "source": [
        "### 7.15.14 - Obter o blob a partir da imagem lida pelo OpenCV, aplicando pré-processamento. O tipo de pré-processamento a ser utilizado irá depender de cada modelo de rede utilizado."
      ],
      "id": "X1JLygvON0gv"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cc52f6b"
      },
      "source": [
        "def getBlob(img, H=416, W=416):\n",
        "    blob = cv2.dnn.blobFromImage(img, 1/255.0, (H, W), swapRB=True, crop=False)\n",
        "    return blob"
      ],
      "id": "1cc52f6b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUJjYYSnO9WY"
      },
      "source": [
        "### 7.15.15 - Definir a imagem lida como entrada para o processamento do modelo e processar a inferência neste modelo, usando a imagem informada como entrada do processo"
      ],
      "id": "IUJjYYSnO9WY"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "968ab582"
      },
      "source": [
        "def netProcessImg(blob, net, ln):\n",
        "    net.setInput(blob)    \n",
        "    results = net.forward(ln)\n",
        "    return results"
      ],
      "id": "968ab582",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26sihY0qPc9x"
      },
      "source": [
        "### 7.15.16 - Desenhar na imagem os marcadores (bounding boxes) para cada uma das classes detectadas"
      ],
      "id": "26sihY0qPc9x"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79b3dcfe"
      },
      "source": [
        "def post_process(img, outputs, conf):\n",
        "    H, W = img.shape[:2]\n",
        "\n",
        "    boxes = []\n",
        "    confidences = []\n",
        "    classIDs = []\n",
        "\n",
        "    for output in outputs:\n",
        "        scores = output[5:]\n",
        "        classID = np.argmax(scores)\n",
        "        confidence = scores[classID]\n",
        "        if confidence > conf:\n",
        "            x, y, w, h = output[:4] * np.array([W, H, W, H])\n",
        "            p0 = int(x - w//2), int(y - h//2)\n",
        "            p1 = int(x + w//2), int(y + h//2)\n",
        "            boxes.append([*p0, int(w), int(h)])\n",
        "            confidences.append(float(confidence))\n",
        "            classIDs.append(classID)\n",
        "            \n",
        "\n",
        "    \n",
        "    indices = cv2.dnn.NMSBoxes(boxes, confidences, conf, conf-0.1)\n",
        "    if len(indices) > 0:\n",
        "        for i in indices.flatten():\n",
        "            (x, y) = (boxes[i][0], boxes[i][1])\n",
        "            (w, h) = (boxes[i][2], boxes[i][3])\n",
        "            color = [int(c) for c in colors[classIDs[i]]]\n",
        "            cv2.rectangle(img, (x, y), (x + w, y + h), color, 2)\n",
        "            text = \"{}: {:.4f}\".format(classes[classIDs[i]], confidences[i])\n",
        "            cv2.putText(img, text, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
        "            \n",
        "            \n",
        "            \n",
        "    return img"
      ],
      "id": "79b3dcfe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-lKqb1xP9HK"
      },
      "source": [
        "### 7.15.17 - Define o processo de detecção usando as funções de apoio defindas acima"
      ],
      "id": "_-lKqb1xP9HK"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4f17d0b7"
      },
      "source": [
        "def detectObjectsInImage(image_dir, image_file, net, ln, H=416, W=416):\n",
        "    img = readImage(os.path.join(image_dir, image_file))  \n",
        "    blob = getBlob(img, H, W)\n",
        "    outputs = netProcessImg(blob, net, ln)\n",
        "    outputs = np.vstack(outputs)    \n",
        "    img = post_process(img, outputs, 0.5)\n",
        "    displayImage(img)"
      ],
      "id": "4f17d0b7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B84i3jWcS7ev"
      },
      "source": [
        "### 7.15.18 - Executar o processo de detecção inicialmente carregando o modelo YOLOV3 em memória"
      ],
      "id": "B84i3jWcS7ev"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5347ef8"
      },
      "source": [
        "# Obter a net YOLOV3\n",
        "net, ln = getYOLONetAndLN(yolov3_dir, yolov3_cfg, yolov3_weights)"
      ],
      "id": "a5347ef8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93d0d5da"
      },
      "source": [
        "image_file = 'giraffe.jpg'\n",
        "detectObjectsInImage(image_dir, image_file, net, ln)\n"
      ],
      "id": "93d0d5da",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diBQD8waTY06"
      },
      "source": [
        "### Atividade para o aluno\n",
        "#### 7.15.19 - Utilizando o modelo  YOLOV3  (desenvolvido com uso do framework Darknet) efetuar a tarefa de detecção de objetos nas imagens definidas abaixo e apresentar os resultados\n",
        "<ul>\n",
        "<li>dog.jpg</li>\n",
        "<li>horses.jpg</li>\n",
        "<li>person.jpg</li>\n",
        "</ul>\n"
      ],
      "id": "diBQD8waTY06"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pi9iy3KIT8vp"
      },
      "source": [
        "### Atividade para o aluno\n",
        "#### 7.15.20 - Utilizando o modelo YOLOV4 (desenvolvido com uso do framework Darknet) efetuar a tarefa de detecção de objetos nas imagens definidas abaixo e apresentar os resultados\n",
        "<ul>\n",
        "<li>horses.jpg</li>\n",
        "<li>giraffe.jpg</li>\n",
        "<li>dog.jpg</li>\n",
        "<li>pexels-engin-akyurt-1769271.jpg</li>\n",
        "<li>pexels-mandie-inman-896567.jpg</li>\n",
        "</ul>"
      ],
      "id": "pi9iy3KIT8vp"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_uQhgdHT8vz"
      },
      "source": [
        "# diretório contendo os arquivos do modelo YOLOV4 \n",
        "yolov4_dir = models_path +  'Darknet/darknet_detect/yolov4'\n",
        "\n",
        "yolov4_cfg = 'yolov4.cfg'\n",
        "yolov4_weights = 'yolov4.weights'\n"
      ],
      "id": "q_uQhgdHT8vz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "587weHslVIXv"
      },
      "source": [
        "# Obter a net YOLOV4 - chamar a função getYOLONetAndLN\n"
      ],
      "id": "587weHslVIXv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Zb369ie41VN"
      },
      "source": [
        "### Atividade para o aluno\n",
        "#### 7.15.21 - Utilizando o modelo YOLOV4-P6 (desenvolvido com uso do framework Darknet) efetuar a tarefa de detecção de objetos nas imagens definidas abaixo e apresentar os resultados\n",
        "<ul>\n",
        "<li>dog.jpg</li>\n",
        "<li>pexels-mandie-inman-896567.jpg</li>\n",
        "</ul>"
      ],
      "id": "7Zb369ie41VN"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEvnivcbycf5"
      },
      "source": [
        "# diretório contendo os arquivos do modelo YOLOV4-P6\n",
        "yolov4p6_dir = models_path +  'Darknet/darknet_detect/yolov4-p6'\n",
        "\n",
        "yolov4p6_cfg = 'yolov4-p6.cfg'\n",
        "yolov4p6_weights = 'yolov4-p6.weights'\n",
        "\n"
      ],
      "id": "EEvnivcbycf5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ko4aDUQ_SxF"
      },
      "source": [
        "# Obter a net YOLOV4-P6 - chamar a função getYOLONetAndLN\n"
      ],
      "id": "7Ko4aDUQ_SxF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTFps3Gq5_HJ"
      },
      "source": [
        "### Atividade para o aluno\n",
        "#### 7.15.22 - Ao executar a atividade acima, é possível que tenha ocorrido um erro:\n",
        "#### ...\\modules\\dnn\\src\\layers\\concat_layer.cpp:102:  error: (-201:Incorrect size of input array) Inconsistent shape for ConcatLayer  in function 'cv::dnn::ConcatLayerImpl::getMemoryShapes'\n",
        "\n",
        "#### Execute novamente essa atividade informando o valor 1280 para os parâmetros de altura e largura da imagem ao chamar a função detectObjectsInImage\n",
        "\n",
        "\n"
      ],
      "id": "PTFps3Gq5_HJ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KgtMRoviaDV"
      },
      "source": [
        "### Atividade para o aluno\n",
        "#### 7.15.23 - Questões\n",
        "<ul>\n",
        "<li>Por que ocorre erro se não informarmos o valor 1280 para altura e largura nos testes realizados no item 7.15.21?</li>\n",
        "<li>Com base nos resultados dos testes realizados para as diferentes imagens, é possível dizer se algum dos modelos vistos é melhor que os demais?</li>\n",
        "</ul>"
      ],
      "id": "-KgtMRoviaDV"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRpClIn6jm3g"
      },
      "source": [
        "### Atividade para o aluno\n",
        "#### 7.15.24 - Atividades extras\n",
        "<ul>\n",
        "<li> Utilizando um dos modelos de detecção vistos até aqui, escolhido a seu critério, efetuar a detecção para todos os arquivos de imagens contidos na pasta (de imagens). O seu código deverá gerar um relatório onde cada linha será composta por: nome do arquivo de imagem, número de objetos detectados, nome da classe 1, nome da classe 2, ..., nome da classe 'n'.\n",
        "Caso uma classe ocorra mais de uma vez para uma dada imagem, informe o nome dessa classe apenas uma vez.\n",
        "\n",
        "Exemplo:\n",
        "\n",
        "dog.jpg, 3, truck, dog, bicycle\n",
        "\n",
        "horses.jpg, 7, horse\n",
        "\n",
        "pexels-engin-akyurt-1769271.jpg, 1, bird\n",
        "\n",
        "\n",
        "<li>Melhorar a atividade descrita acima, gravando em outra pasta de sua escolha novos arquivos contendo as imagens resultantes da detecção (com as 'bounding-boxes',etc.). Cada arquivo gerado terá como nome a classe detectada com o maior índice de confiabilidade obtido, se houver empates escolha qualquer uma das melhores colocadas. Por exemplo, o arquivo pexels-engin-akyurt-1769271 apresenta uma detecção de bird. Logo, você deverá gravar a imagem gerada com as detecçoes (bounding-boxes) no novo arquivo bird.jpg. Verifique antes se já não existe um arquivo com o mesmo nome na pasta. Se ocorrer essa situação, adicionar um sufixo sequencial ao nome do arquivo ('_n'): bird_1.jpg, bird_2.jpg, etc. \n",
        "</li>\n",
        "</ul>\n",
        "\n",
        "#### Para ajudar no desenvolvimento dessas atividades, apresento abaixo exemplos de código para criar uma pasta, gravar e ler um arquivo no ambiente do Google Colab. Os arquivos serão armazenados no seu Google Drive."
      ],
      "id": "lRpClIn6jm3g"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaAgP9zi4E6b"
      },
      "source": [
        "mydrivepath = \"gdrive/MyDrive\""
      ],
      "id": "jaAgP9zi4E6b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1tI_9On4Gn2"
      },
      "source": [
        "outimage = 'opencv_workshop_saida_imagens'\n",
        "outdir = os.path.join(mydrivepath, outimage)\n",
        "if not os.path.exists(outdir):\n",
        "    os.mkdir(outdir)\n"
      ],
      "id": "P1tI_9On4Gn2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aALujFGi4p8R"
      },
      "source": [
        "# Write line to file\n",
        "file = 'Example1.txt'\n",
        "opfile = os.path.join(outdir, file)  \n",
        "with open(opfile, 'w') as wfile:\n",
        "    wfile.write(\"This is line Example1.txt\")"
      ],
      "id": "aALujFGi4p8R",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyHGnFOi4-77"
      },
      "source": [
        "# Read file\n",
        "with open(opfile, 'r') as rfile:\n",
        "    print(rfile.read())"
      ],
      "id": "SyHGnFOi4-77",
      "execution_count": null,
      "outputs": []
    }
  ]
}